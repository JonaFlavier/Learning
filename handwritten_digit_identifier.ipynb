{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonaFlavier/Learning/blob/main/handwritten_digit_identifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN4iq1mUiOQ9"
      },
      "source": [
        "# **Exploring Neural Networks: Image Classifier**\n",
        "\n",
        "This notebook documents the **my exploration into neural networks**, starting with a **simple handwritten digit classifier** using the MNIST dataset. The goal is to **understand the fundamental building blocks** of a neural network, including:\n",
        "\n",
        "- **Forward propagation**: How inputs flow through layers to produce predictions.\n",
        "- **Backpropagation**: How errors are propagated backward to adjust weights.\n",
        "- **Gradient descent**: The optimization process to improve model accuracy.\n",
        "\n",
        "## **Why Start with MNIST?**\n",
        "MNIST is a widely used dataset for handwritten digit recognition, making it a **great starting point** for understanding deep learning fundamentals. It consists of **28×28 grayscale images** labeled from **0 to 9**.\n",
        "\n",
        "## **What This Notebook Covers**\n",
        "- Implementing a **fully connected neural network** from scratch.\n",
        "- Training the model using **Stochastic Gradient Descent (SGD)**.\n",
        "- Evaluating initial results and identifying areas for improvement.\n",
        "- Laying the foundation for future optimizations and experiments.\n",
        "\n",
        "As I progress, I will explore techniques to **enhance performance**, such as **different activation functions, weight initialization strategies, and optimization algorithms**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT5u2xBhiuNK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BuwTjMEekNuH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating the neural network**\n",
        "### The class Network explores the underlying process used to train models.\n",
        "\n",
        "### This model uses the most basic methods such as Stochastic Gradient Descent (SGD) for backpropagation, sigmoid for the activation function, the cost derivative to measure errors between iterations to help with updating weights and biases\n",
        "---\n",
        "## **Formulas applied:**\n",
        "### **Feedforward propagation:**\n",
        "\n",
        "$$\n",
        "z = W \\cdot a + b\n",
        "$$\n",
        "$$\n",
        "a' = \\sigma(z)\n",
        "$$\n",
        "\n",
        "- Each neuron computes a weighted sum of its inputs.\n",
        "- The weighted sum \\( z \\) is passed through the **sigmoid activation function**.\n",
        "\n",
        "## **Sigmoid Activation function:**\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "### **Derivative (used in backpropagation):**\n",
        "$$\n",
        "\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
        "$$\n",
        "- The sigmoid function maps inputs to values between 0 and 1.\n",
        "- The derivative is used for adjusting weights during backpropagation.\n",
        "\n",
        "## **Backpropagation:**\n",
        "### **Output Layer Error:**\n",
        "$$\n",
        "\\delta_L = (a_L - y) \\odot \\sigma'(z_L)\n",
        "$$\n",
        "\n",
        "### **Hidden Layer Error Propagation:**\n",
        "$$\n",
        "\\delta^l = (W^{l+1})^T \\cdot \\delta^{l+1} \\odot \\sigma'(z^l)\n",
        "$$\n",
        "- The error propagates backward through the network using the derivative of the activation function.\n",
        "\n",
        "\n",
        "\n",
        "## **Stochastic Gradient Descent (SGD):**\n",
        "$$\n",
        "W = W - \\frac{\\eta}{m} \\sum \\nabla W\n",
        "$$\n",
        "$$\n",
        "b = b - \\frac{\\eta}{m} \\sum \\nabla b\n",
        "$$\n",
        "- $\\eta$ is the learning rate.\n",
        "- $m$ is the batch size.\n",
        "- Gradients are accumulated over the mini-batch and used to update the weights and biases.\n",
        "\n",
        "\n",
        "## **Cost Function Derivative:**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial C}{\\partial a} = (a_{\\text{output}} - y)\n",
        "$$\n",
        "- Measures the error between predicted output $ a_{\\text{output}} $ and actual label $ y $.\n",
        "- This derivative helps update the model parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "vcWC6gcsF9s8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "W54gfLvzkml7"
      },
      "outputs": [],
      "source": [
        "# create a network object\n",
        "\n",
        "class Network (object):\n",
        "  def __init__(self, sizes):\n",
        "    \"\"\"\n",
        "    sizes -> list containing the number of neurons per layer\n",
        "    i.e.: [2,4,3,1] => input layer has 2 neurons, layer 2 has 4 neurons and layer 3 has 3 inner neurons whilst layer 4 has 1 neuron\n",
        "    each neuron will have a random weight assigned initially which will be manipulated at each iteration of SGD\n",
        "    Assume that the input layer will not require a bias as its only required in the outer layer neurons\n",
        "\n",
        "    the weight matrix will be for connecting => (destination layer, source layer)\n",
        "      (4 neuron layer, 2 neuron layer),\n",
        "      (3 neuron layer, 4 neuron layer),\n",
        "      (1 neuron layer, 3 neuron layer)\n",
        "\n",
        "    \"\"\"\n",
        "    self.num_layers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    self.biases = [np.random.randn(y,1) for y in sizes[1:]] # for every layer after the input layer in sizes, attach a random bias\n",
        "    self.weights = [np.random.randn(y,x) for x, y in zip(sizes[:-1], sizes[1:])] # the weight matrix will be for connecting => (4,2), (3,4), (1,3)\n",
        "    print(f\"biases {len(self.biases)} {self.biases[0].shape} {self.biases[1].shape}\")\n",
        "    print(f\"weights {len(self.weights)} {self.weights[0].shape} {self.weights[1].shape}\")\n",
        "\n",
        "  def sigmoid(self,z):\n",
        "    # function is defined by => σ(z) = 1/1+e^-z\n",
        "    return 1.0/(1.0 + np.exp(-z))\n",
        "\n",
        "  def sigmoid_prime(self, z):\n",
        "    #\n",
        "    return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "  def feedforward(self, a):\n",
        "    # apply the activation function per layer => a = σ(weight*input + bias)\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      a = self.sigmoid(np.dot(w, a)+b)\n",
        "    return a\n",
        "\n",
        "  def SGD (self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "    \"\"\"\n",
        "    Training neural network: Stochastic Gradient Descent\n",
        "    training_data = list of tuples (training inputs X, desired outputs Y)\n",
        "    test_data = if provided will evaluate against test data after each epoch\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"{len(training_data)}\")\n",
        "    if test_data is not None: n_test = len(test_data) # if present get the number of test data rows\n",
        "    n = len(training_data) # get the length of the data rows being trained\n",
        "\n",
        "    for i in range(epochs):\n",
        "      # shuffle the training data\n",
        "      random.shuffle(training_data)\n",
        "      # segregate into mini batches\n",
        "      mini_batches = [\n",
        "          training_data[k:k+mini_batch_size]\n",
        "          for k in range(0, n, mini_batch_size)\n",
        "      ]\n",
        "\n",
        "      # update per mini batch\n",
        "      for mini_batch in mini_batches:\n",
        "        self.update_mini_batch(mini_batch, eta)\n",
        "\n",
        "      # evaluate iteration if test data is present\n",
        "      if test_data:\n",
        "        print(f\"Epoch {i}: {self.evaluate(test_data)}/ {n_test}\")\n",
        "      else:\n",
        "        print(f\"Epoch {i} complete\")\n",
        "\n",
        "  def update_mini_batch(self, mini_batch, eta):\n",
        "    \"\"\"\n",
        "    updates weights and biases using gradient descent\n",
        "    backpropagation is used per mini batch\n",
        "    mini_batch => list of tuples \"(x, y)\"\n",
        "    eta => learning_rate\n",
        "    \"\"\"\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases] # list of zero matrices to store sum of gradients of the bias for all samples in minibatch\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights] # list of zero matrices to store sum of gradient of the weights for all samples in minibatch\n",
        "\n",
        "    for x, y in mini_batch:\n",
        "      delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "      nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] # apply the changes per bias in each layer by the delta\n",
        "      nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] # apply the changes per weight in each layer by the delta\n",
        "\n",
        "    # use the function x-(alpha/batch_size)*x' to apply the learning rate\n",
        "    self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] # set the new biases from this mini_batch\n",
        "    self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] # set the new weights from this mini_batch\n",
        "\n",
        "  def backprop(self, x, y):\n",
        "    # print(f\"x shape{x.shape} y shape{y.shape}\")\n",
        "    x = x.reshape(-1,1)\n",
        "    y = y.reshape(-1,1)\n",
        "    nabla_b =[np.zeros(b.shape) for b in self.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "    # feed forward\n",
        "    activation = x\n",
        "    activations = [x] # list for all activations\n",
        "    zs = [] # list storing all z vectors\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      z = np.dot(w, activation)+b # using the function w*input + b\n",
        "      # print(f\"z value {z}\")\n",
        "      zs.append(z) # store weighted sum before activation\n",
        "      # print(f\" zs length {len(zs)} {zs[0].shape}\")\n",
        "      activation = self.sigmoid(z) # apply activation function\n",
        "      activations.append(activation) # collect activation result into list\n",
        "\n",
        "    # backward pass\n",
        "    delta = self.cost_derivative(activations[-1],y) * self.sigmoid_prime(zs[-1]) # calculate the difference between the network output layer and the true results\n",
        "    # print(f\"delta shape: {delta.shape}\")\n",
        "    delta = delta.reshape(-1, 1)  # Ensure delta is a column vector\n",
        "\n",
        "    # print(f\"sigmoid prime: {self.sigmoid_prime(zs[-1])}\")\n",
        "\n",
        "    nabla_b[-1] = delta # gradient of cost function with respect to biases in output layer\n",
        "    # print(f\"nabla_b delta shape{nabla_b[-1].shape}\")\n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].T) # gradient for cost function wrt weights in output layer by calculation how much of the weight contributed to the error\n",
        "    # print(f\"nabla_w[-1] shape{nabla_w[-1].shape}\")\n",
        "\n",
        "    for l in range(2, self.num_layers):\n",
        "      z = zs[-l] # get weighted sum for layer 1\n",
        "      sp = self.sigmoid_prime(z) # get sigmoid derivative\n",
        "      delta = np.dot(self.weights[-l+1].T, delta) * sp # backpropagates error by multiplying the weights with the transpose of the weight matric\n",
        "\n",
        "      activations_prev = activations[-l-1].reshape(-1,1)\n",
        "      # print(f\"delta shape: {delta.shape}\")\n",
        "      # print(f\"activations[-l-1] shape: {activations_prev.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "      nabla_b[-l] = delta # store bias for current layer\n",
        "      nabla_w[-l] = np.dot(delta, activations_prev.T) # store bias for current layer\n",
        "\n",
        "    return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "  def evaluate(self, test_data):\n",
        "    \"\"\"\n",
        "    store in a tuple list the inference result and the actual result in (prediction, true_result) format\n",
        "    then sum up which inferences were equal to the true result\n",
        "    \"\"\"\n",
        "\n",
        "    test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
        "\n",
        "    return sum(int(x.item()==y.item()) for (x, y) in test_results)\n",
        "\n",
        "\n",
        "\n",
        "  def cost_derivative(self, output_activations, y):\n",
        "    \"\"\"\n",
        "    get vector of partial derivatives \\ partial C_x \\ partial a for the output activations\n",
        "    \"\"\"\n",
        "    return (output_activations-y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQkqFo9sadba"
      },
      "outputs": [],
      "source": [
        "# pull the data from mnist and feed training data and test data into the model\n",
        "(x_train,y_train), (x_test, y_test) = mnist.load_data() # get the data\n",
        "# print(f\"{x_train.shape}\")\n",
        "# print(f\"{y_train.shape}\")\n",
        "# print(f\"{x_test.shape}\")\n",
        "# print(f\"{y_test.shape}\")\n",
        "\n",
        "# normalise the input data to values 0-1\n",
        "x_train = x_train.astype('float32')/255.0\n",
        "x_test = x_test.astype('float32')/255.0\n",
        "\n",
        "#flatten images from 2d (28, 28) to 1d (784,)\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "print(f\"{x_train.shape}\")\n",
        "print(f\"{y_train.shape}\")\n",
        "print(f\"{x_test.shape}\")\n",
        "print(f\"{y_test.shape}\")\n",
        "\n",
        "# convert to list\n",
        "\n",
        "\n",
        "training_data = list(zip(x_train, y_train))\n",
        "test_data = list(zip(x_test, y_test))\n",
        "\n",
        "net = Network([784, 100,10]) # network with 784 neurons in the 1st input layer, 30 neurons in the 2nd inner layer and 10 in the 3rd layer\n",
        "\n",
        "\n",
        "net.SGD(training_data, 50, 10, 5.0, test_data=test_data)\n",
        "print(\"End of training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initial Evaluation and Observations**\n",
        "\n",
        "After implementing the base neural network, the initial results suggest that performance is **not yet optimal**. Below are some key observations from the current implementation.\n",
        "\n",
        "## **1️. Observed Issues**\n",
        "- **Inconsistent training results** – Accuracy varies significantly across runs.\n",
        "- **Slow convergence** – The network takes many epochs to reach reasonable accuracy.\n",
        "- **Potential vanishing gradient problem** – Sigmoid activation may cause small gradient updates.\n",
        "- **Weight initialization concerns** – Random initialization might be affecting stability.\n",
        "- **Possible overfitting** – Training accuracy is much higher than test accuracy.\n",
        "\n",
        "## **2. Areas for Further Investigation**\n",
        "To improve the network’s performance, the following areas will be explored:\n",
        "- **Weight Initialization:** Investigate whether **Xavier (Glorot) Initialization** helps stabilize training.\n",
        "- **Activation Functions:** Compare **ReLU vs. Sigmoid** to address gradient vanishing.\n",
        "- **Loss Function:** Evaluate whether **Cross-Entropy Loss** is more effective than MSE for classification.\n",
        "- **Optimizer Choices:** Test **Adam or Momentum-based SGD** for faster convergence.\n",
        "- **Regularization Techniques:** Consider using **L2 Regularization or Dropout** to reduce overfitting.\n",
        "\n",
        "## **3️. Next Steps**\n",
        "- Run additional experiments to analyze how weight initialization affects training stability.\n",
        "- Compare different activation functions and their impact on gradient flow.\n",
        "- Implement and test different optimization strategies.\n",
        "- Evaluate the effects of regularization on generalization.\n",
        "\n",
        "The next phase will involve **testing these modifications one by one** to analyze their impact on performance.\n"
      ],
      "metadata": {
        "id": "2YnCvDsvNFq0"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9pAfijyyqtAhl0r9rqohB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}